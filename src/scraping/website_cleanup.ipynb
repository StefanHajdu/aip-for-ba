{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/config/anaconda3/envs/py12/lib/python3.12/html/parser.py:171: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    }
   ],
   "source": [
    "# extract paragraphs from .html files\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "DATA_PATH = \"/workspace/aip-for-ba/data\"\n",
    "prefix = \"\"\"\n",
    "Hlavné mesto Slovenskej republiky Bratislava\n",
    "Primaciálne námestie 1\n",
    "814 99 Bratislava\n",
    "\n",
    "IČO: 00603481\n",
    "DIČ: 2020372596\n",
    "IČ DPH: SK2020372596\n",
    "Mesto Bratislava\n",
    "Rýchle odkazy\n",
    "English\n",
    "Hlavné mesto Slovenskej republiky Bratislava\n",
    "Primaciálne námestie 1\n",
    "814 99 Bratislava\n",
    "\n",
    "IČO: 00603481\n",
    "DIČ: 2020372596\n",
    "IČ DPH: SK2020372596\n",
    "English\"\"\"\n",
    "\n",
    "def extract_paragraphs(html_file):\n",
    "    with open(html_file, 'r') as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        return soup.find(\"title\"), paragraphs\n",
    "\n",
    "\n",
    "data = []\n",
    "Path(f\"{DATA_PATH}/paragraphs\").mkdir(parents=True, exist_ok=True)\n",
    "for _file in Path(f\"{DATA_PATH}/domains/\").rglob(\"*.html\"):\n",
    "    if _file.is_dir():\n",
    "        continue\n",
    "    paragraphs = extract_paragraphs(_file)\n",
    "    url = \".\".join(_file.parts[5:-1])\n",
    "    with open(f'{DATA_PATH}/paragraphs/{url}.txt', 'w') as f:\n",
    "        if not paragraphs[0]:\n",
    "            continue\n",
    "        data.append({\n",
    "            \"title\": paragraphs[0].get_text(),\n",
    "            \"paragraph\": \"\\n\".join([p.get_text() for p in paragraphs[1]]).replace(prefix, \"\"),\n",
    "            \"source\": f\"https://{url}\",\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{DATA_PATH}/ingest_data.json\", 'w') as f:\n",
    "    f.write(json.dumps(data, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
